{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Differentiation Using Pytorch\n",
        "\n",
        "Author: Muhammed Ashrah\n",
        "\n",
        "Source Adapted From: Dive into Deep Learning (d2.ail)\n",
        "\n",
        "ðŸ“ This notebook is adapted from the \"Automatic Differentiation\" section of the Dive into Deep Learning book.\n",
        "\n",
        "This notebook provides a beginner-friendly walkthrough of differentiation concepts using PyTorch.\n",
        "\n",
        "> ðŸ’¡ The goal is not just to run the code, but to understand whatâ€™s happening behind the scenes.\n",
        "\n",
        "Feel free to tweak the examples and experiment!\n",
        "\n"
      ],
      "metadata": {
        "id": "YiWP-EdOlzSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh1Xx6_e_uoR"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating differentiation of a simple function when given input input is scalar"
      ],
      "metadata": {
        "id": "yT4h3cNPHLXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.arange(6,dtype=float,requires_grad=True) # This requires_grad helps us to calculate gradient\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U562IIc_8wt",
        "outputId": "3a5669fe-a9e0-4961-b439-c05e42d41d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3., 4., 5.], dtype=torch.float64, requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to find the gradient of : ![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "SnjfGGfuAfbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us begin by finding y using 2 * dot product of (x,x)\n",
        "y=2*torch.dot(x,x)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn73ETD3FfmR",
        "outputId": "2426d310-16d7-4521-ed33-748dec12236b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(110., dtype=torch.float64, grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us now find the derivative . For this , first you have to do y.backward()\n",
        "# This is the step in which dy/dx is being calculated\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "nuQmA9C0F09Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are applying exact value of x into dy/dx in this step\n",
        "x.grad # We know that dy/dx will be 4*x, so the values must just be x vector with 4 multiplied to each element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxwE711UG2CA",
        "outputId": "ce569ca7-7663-4a10-9972-90c6fdbb462f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12., 16., 20.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch does not automatically reset the gradient buffer when we record a new gradient. Instead, the new gradient is added to the already-stored gradient"
      ],
      "metadata": {
        "id": "gfLzlEZkH1pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To reset\n",
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ8PdaJPHcrP",
        "outputId": "58b63b79-fee0-4bff-d91a-b965ef84ea6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=x.sum()\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyxo4YV3H8_1",
        "outputId": "ad694d90-99dd-4875-ba18-0eb80e74c007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(15., dtype=torch.float64, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us now find the derivative\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTvM_94iIJq9",
        "outputId": "cd887927-7af5-4d01-de59-1f38110dc93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1., 1.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating gradient for non-scalar variables"
      ],
      "metadata": {
        "id": "OTAv1y5MI0mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJgR65sTJA5S",
        "outputId": "f9798bb6-d1c4-4ea8-c6fe-3508a49fb656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=x*x\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut7L5R-sJHwL",
        "outputId": "83c3196b-39ee-4d1e-8582-d7e68c7fa2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  4.,  9., 16., 25.], dtype=torch.float64,\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us now find the derivative\n",
        "y.sum().backward()\n"
      ],
      "metadata": {
        "id": "HLH8t8ouJQrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_srMydIJrX3",
        "outputId": "c28abf88-685c-41a1-8142-48b1f691969c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  2.,  4.,  6.,  8., 10.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detaching Computation\n"
      ],
      "metadata": {
        "id": "PLPX4BnUMi4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the input to create some auxiliary intermediate terms for which we do not want to compute a gradient. In this case, we need to detach the respective computational graph from the final result."
      ],
      "metadata": {
        "id": "huGnTDCjMpK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS2BTtNnMn92",
        "outputId": "fbcea498-d37c-4cf4-fa18-bcf828bd0ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=x*x"
      ],
      "metadata": {
        "id": "nw2wC0uONCnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u=y.detach() # By detaching , we remove y from computation graph so u will have not remember how y was created. It just gets the values of y\n",
        "# In simple words , now u will be treated as a constant in differentiation step"
      ],
      "metadata": {
        "id": "JLJSr0F4NEsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z=u*x\n",
        "Z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeRhUPSZNtIS",
        "outputId": "4488b4bc-b12a-47f4-fe9b-7b1a30c064a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0.,   1.,   8.,  27.,  64., 125.], dtype=torch.float64,\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z.sum().backward()\n"
      ],
      "metadata": {
        "id": "5ufUsCc7N5pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWyHsgoYOFsB",
        "outputId": "7a60e73d-f642-47b0-e91f-444cb69cf080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  4.,  9., 16., 25.], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}